{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Roboflow-Custom-YOLOv5",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# How to Train YOLOv5 to Count Fruit On Trees with synthetic data\n",
        "\n",
        "\n",
        "\n",
        "This notebook is based on Jacob Solawetz Tutorial on training custom objects with YOLOv5\n",
        "\n",
        "In order to do so he used the [YOLOv5 repository](https://github.com/ultralytics/yolov5) by [Ultralytics](https://www.ultralytics.com/). \n",
        "\n",
        "\n",
        "\n",
        "I hope it helps others trying to handle creating datasets for specific features with out a proper dataset.\n",
        "\n",
        "The notebook is geared towards training a dataset created of synthetic data to recognize real objects.\n",
        "You can find the [here](https://medium.com/p/dab5728f6411)\n",
        "\n",
        "In this notebook I will skim over the issues discussed in\n",
        "Jacob Solawetz blog post and simply add the needed cells as they are if you want further information regarding the use I will suggest [reading through his blog post](https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "#Preperation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie5uLDH4uzAp"
      },
      "source": [
        "# clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "source": [
        "%cd yolov5\n",
        "# install dependencies\n",
        "!pip install -qr requirements.txt  # as suggested in the tutorial you can ignore errors\n",
        "import torch\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.google_utils import gdrive_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtY4Fyz_TYBh"
      },
      "source": [
        "Make sure that as a result you see a cuda device in my case I got 'Tesla P100-PCIE-16GB'. \n",
        "\n",
        "But you may get another GPU such as Tesla K80'\n",
        "\n",
        "If not go to Runtime -> Change runtime type and change it to GPU \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmiQzsiRTxl8"
      },
      "source": [
        "#downloading the dataset \n",
        "If you are using Roboflow set the url and run the next 2 cells.\n",
        "\n",
        "If you decide to use the dataset in a different way this is where you should set it up\n",
        "\n",
        "As a result make sure you have 2 librarires \n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "\n",
        "As well as a data.yaml file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jKM6GfzlgpS"
      },
      "source": [
        "#@title Set your Roboflow url here\n",
        "url = 'https://app.roboflow.com/ds/nisrp51sC0?key=1FIctqMRYN' #@param {type:\"string\"}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knxi2ncxWffW"
      },
      "source": [
        "# Export code snippet and paste here\n",
        "%cd /content\n",
        "print(url)\n",
        "!curl -L $url > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiv_r-oMnLnG"
      },
      "source": [
        "If instead you want to use my Dataset run the following 2 cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs_mVAlul9Di"
      },
      "source": [
        "!git clone https://github.com/Amizorach/FruitSyntheticDataset\n",
        "%cd /content\n",
        "!unzip FruitSyntheticDataset/dataset/ds.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJx-2NHsYxT"
      },
      "source": [
        "# Define Model Configuration and Architecture\n",
        "\n",
        "This is the default setup suggested in  Jacob Solawetz \n",
        "I did not change it - but as he points out you can.\n",
        "I intend to change it and test different configurations later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rvt5wilnDyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c230ae-46c5-441d-d43a-09ab479d1322"
      },
      "source": [
        "import yaml\n",
        "%cd /content\n",
        "with open(\"data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t14hhyqdmw6O"
      },
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDxebz13RdRA"
      },
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MULxzUWOWWq"
      },
      "source": [
        "Before training lets check the options \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "\n",
        "\n",
        "# Training\n",
        "You can easily check what paramaters Ultralytics accepts running the following cell \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC00zOJ6OsxV",
        "outputId": "05edc490-4826-4063-ec78-f86801233845"
      },
      "source": [
        "%cd /content/yolov5/\n",
        "!python train.py -h"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\n",
            "                [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
            "                [--img-size IMG_SIZE [IMG_SIZE ...]] [--rect]\n",
            "                [--resume [RESUME]] [--nosave] [--notest] [--noautoanchor]\n",
            "                [--evolve] [--bucket BUCKET] [--cache-images]\n",
            "                [--image-weights] [--device DEVICE] [--multi-scale]\n",
            "                [--single-cls] [--adam] [--sync-bn] [--local_rank LOCAL_RANK]\n",
            "                [--workers WORKERS] [--project PROJECT] [--entity ENTITY]\n",
            "                [--name NAME] [--exist-ok] [--quad] [--linear-lr]\n",
            "                [--label-smoothing LABEL_SMOOTHING] [--upload_dataset]\n",
            "                [--bbox_interval BBOX_INTERVAL] [--save_period SAVE_PERIOD]\n",
            "                [--artifact_alias ARTIFACT_ALIAS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --weights WEIGHTS     initial weights path\n",
            "  --cfg CFG             model.yaml path\n",
            "  --data DATA           data.yaml path\n",
            "  --hyp HYP             hyperparameters path\n",
            "  --epochs EPOCHS\n",
            "  --batch-size BATCH_SIZE\n",
            "                        total batch size for all GPUs\n",
            "  --img-size IMG_SIZE [IMG_SIZE ...]\n",
            "                        [train, test] image sizes\n",
            "  --rect                rectangular training\n",
            "  --resume [RESUME]     resume most recent training\n",
            "  --nosave              only save final checkpoint\n",
            "  --notest              only test final epoch\n",
            "  --noautoanchor        disable autoanchor check\n",
            "  --evolve              evolve hyperparameters\n",
            "  --bucket BUCKET       gsutil bucket\n",
            "  --cache-images        cache images for faster training\n",
            "  --image-weights       use weighted image selection for training\n",
            "  --device DEVICE       cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
            "  --multi-scale         vary img-size +/- 50%\n",
            "  --single-cls          train multi-class data as single-class\n",
            "  --adam                use torch.optim.Adam() optimizer\n",
            "  --sync-bn             use SyncBatchNorm, only available in DDP mode\n",
            "  --local_rank LOCAL_RANK\n",
            "                        DDP parameter, do not modify\n",
            "  --workers WORKERS     maximum number of dataloader workers\n",
            "  --project PROJECT     save to project/name\n",
            "  --entity ENTITY       W&B entity\n",
            "  --name NAME           save to project/name\n",
            "  --exist-ok            existing project/name ok, do not increment\n",
            "  --quad                quad dataloader\n",
            "  --linear-lr           linear LR\n",
            "  --label-smoothing LABEL_SMOOTHING\n",
            "                        Label smoothing epsilon\n",
            "  --upload_dataset      Upload dataset as W&B artifact table\n",
            "  --bbox_interval BBOX_INTERVAL\n",
            "                        Set bounding-box image logging interval for W&B\n",
            "  --save_period SAVE_PERIOD\n",
            "                        Log model after every \"save_period\" epoch\n",
            "  --artifact_alias ARTIFACT_ALIAS\n",
            "                        version of dataset artifact to be used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qB6AFsSPB26"
      },
      "source": [
        "Lets collect the information needed from what we did up to now.\n",
        "\n",
        "- **data:** is the data.yaml downloaded in our case from roboflow and should be in the main directory (../data.yaml)\n",
        "\n",
        "- **weights:** leaving this empty ('') will run yolov5/weights/download_weights.sh and download the config for you\n",
        "\n",
        "- **img:** You will need to suplly the following according to how you created the dataset - in my case I created images of size (250,250) but converted them using roboflow to (416, 416) and so this is what I supply\n",
        "\n",
        "- **batch:** choose your batch size - higher batch sizes leads to lower asymptotic test accuracy. you can read more about batchsizes in [Kevin Shen's article]( https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e#:~:text=Training%20loss%20and%20accuracy%20when,trained%20using%20different%20batch%20sizes.&text=Finding%3A%20higher%20batch%20sizes%20leads,number%20of%20epochs%20of%20training.)\n",
        "\n",
        "- **epochs:** you should not need many I ran it for 100 epochs but it seemed to have converged after around 40.\n",
        "\n",
        "- **cfg:** this is where you pick the model configuration I choose yolov5s because of its speed \n",
        "you can find more information [here](https://github.com/ultralytics/yolov5#pretrained-checkpoints). In any case all the configurations can be found in yolov5/models\n",
        "\n",
        "- **name:** choose your result names\n",
        "\n",
        "\n",
        "My final output was directed by the original notebook and looks like the folowin cell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "source": [
        "# time its performance\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 416 --batch 64 --epochs 100 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights ''  --cache-images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVs_4zEeVbF"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXYLeYxsVHUr"
      },
      "source": [
        "After testing you can find the results in runs/results.txt (unless you changed its name)\n",
        "\n",
        "You can see the results using Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQolp5bzVslT"
      },
      "source": [
        "As a result you should find 2 sets of weights in \n",
        "runs/train/yolov5s_results/weights/ --\n",
        "best.pt and last.pt that most probably correspond to their names\n",
        "\n",
        "We can run infference using the following cell\n",
        "simply set the folowing:\n",
        "\n",
        "- **weights:** simply choose runs/train/yolov5s_results/weights/best.pt\n",
        "\n",
        "- **img:** the image size of the training dataset (not neccarly the same as what you are testing\n",
        "\n",
        "- **conf:** the confidince you want to use for detection the higher the confidance the more precise a detection needs to be\n",
        "\n",
        "- **source:** a path to a image (if you suplly a directory it will collect all images in the directory). For startes you can use the test directory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nmZZnWOgJ2S"
      },
      "source": [
        "# use the best weights!\n",
        "%cd /content/yolov5/\n",
        "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source ../test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkMWVSJDXJP6"
      },
      "source": [
        "After running the code it will print where it saved the images. The message should look similar to \n",
        "\"Results saved to runs/detect/exp5\"\n",
        "\n",
        "You can simply go to the files saved in this directory and double click a image to see the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odKEqYtTgbRc"
      },
      "source": [
        "#display inference on ALL test images\n",
        "#this looks much better with longer training above\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVpCFeU-K4gb"
      },
      "source": [
        "## Before I finish\n",
        "\n",
        "I cant stress enough that this is a simplified version of [Roboflow's notebook](https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ#scrollTo=dOPn9wjOAwwK)\n",
        "\n",
        "It is not as complete and intended mainly for directing from my blog regarding the creation of synthetic data.\n",
        "I would like to thank them for the orignal notebook an incourage you to check it out"
      ]
    }
  ]
}